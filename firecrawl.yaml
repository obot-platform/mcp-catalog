name: Firecrawl
description: |
  # Firecrawl MCP Server

  A Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/mendableai/firecrawl) for web scraping capabilities.

  ## Features

  - Web scraping, crawling, and discovery
  - Search and content extraction
  - Deep research and batch scraping
  - Automatic retries and rate limiting
  - SSE support

  ## Configuration

  - `FIRECRAWL_API_KEY`: Your Firecrawl API key
    - Required when using cloud API (default)

  ## Rate Limiting and Batch Processing

  The server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:

  - Automatic rate limit handling with exponential backoff
  - Efficient parallel processing for batch operations
  - Smart request queuing and throttling
  - Automatic retries for transient errors

  ## How to Choose a Tool

  Use this guide to select the right tool for your task:

  - **If you know the exact URL(s) you want:**
    - For one: use **scrape**
    - For many: use **batch_scrape**
  - **If you need to discover URLs on a site:** use **map**
  - **If you want to search the web for info:** use **search**
  - **If you want to extract structured data:** use **extract**
  - **If you want to analyze a whole site or section:** use **crawl** (with limits!)
  - **If you want to do in-depth research:** use **deep_research**
  - **If you want to generate LLMs.txt:** use **generate_llmstxt**

  ### Quick Reference Table

  | Tool                | Best for                                 | Returns         |
  | ------------------- | ---------------------------------------- | --------------- |
  | scrape              | Single page content                      | markdown/html   |
  | batch_scrape        | Multiple known URLs                      | markdown/html[] |
  | map                 | Discovering URLs on a site               | URL[]           |
  | crawl               | Multi-page extraction (with limits)      | markdown/html[] |
  | search              | Web search for info                      | results[]       |
  | extract             | Structured data from pages               | JSON            |
  | deep_research       | In-depth, multi-source research          | summary, sources|
  | generate_llmstxt    | LLMs.txt for a domain                    | text            |

  ## Available Tools

  ### 1. Scrape Tool (`firecrawl_scrape`)

  Scrape content from a single URL with advanced options.

  **Best for:**
  - Single page content extraction, when you know exactly which page contains the information.

  **Not recommended for:**
  - Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)
  - When you're unsure which page contains the information (use search)
  - When you need structured data (use extract)

  **Common mistakes:**
  - Using scrape for a list of URLs (use batch_scrape instead).

  **Prompt Example:**

  > "Get the content of the page at https://example.com."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_scrape",
    "arguments": {
      "url": "https://example.com",
      "formats": ["markdown"],
      "onlyMainContent": true,
      "waitFor": 1000,
      "timeout": 30000,
      "mobile": false,
      "includeTags": ["article", "main"],
      "excludeTags": ["nav", "footer"],
      "skipTlsVerification": false
    }
  }
  ```

  **Returns:**
  - Markdown, HTML, or other formats as specified.

  ### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)

  Scrape multiple URLs efficiently with built-in rate limiting and parallel processing.

  **Best for:**
  - Retrieving content from multiple pages, when you know exactly which pages to scrape.

  **Not recommended for:**
  - Discovering URLs (use map first if you don't know the URLs)
  - Scraping a single page (use scrape)

  **Common mistakes:**
  - Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)

  **Prompt Example:**

  > "Get the content of these three blog posts: [url1, url2, url3]."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_batch_scrape",
    "arguments": {
      "urls": ["https://example1.com", "https://example2.com"],
      "options": {
        "formats": ["markdown"],
        "onlyMainContent": true
      }
    }
  }
  ```

  **Returns:**
  - Response includes operation ID for status checking:

  ```json
  {
    "content": [
      {
      "type": "text",
      "text": "Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress."
      }
    ],
    "isError": false
  }
  ```

  ### 3. Check Batch Status (`firecrawl_check_batch_status`)

  Check the status of a batch operation.

  ```json
  {
    "name": "firecrawl_check_batch_status",
    "arguments": {
      "id": "batch_1"
    }
  }
  ```

  ### 4. Map Tool (`firecrawl_map`)

  Map a website to discover all indexed URLs on the site.

  **Best for:**
  - Discovering URLs on a website before deciding what to scrape
  - Finding specific sections of a website

  **Not recommended for:**
  - When you already know which specific URL you need (use scrape or batch_scrape)
  - When you need the content of the pages (use scrape after mapping)

  **Common mistakes:**
  - Using crawl to discover URLs instead of map

  **Prompt Example:**

  > "List all URLs on example.com."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_map",
    "arguments": {
      "url": "https://example.com"
    }
  }
  ```

  **Returns:**
  - Array of URLs found on the site

  ### 5. Search Tool (`firecrawl_search`)

  Search the web and optionally extract content from search results.

  **Best for:**
  - Finding specific information across multiple websites, when you don't know which website has the information.
  - When you need the most relevant content for a query

  **Not recommended for:**
  - When you already know which website to scrape (use scrape)
  - When you need comprehensive coverage of a single website (use map or crawl)

  **Common mistakes:**
  - Using crawl or map for open-ended questions (use search instead)

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_search",
    "arguments": {
      "query": "latest AI research papers 2023",
      "limit": 5,
      "lang": "en",
      "country": "us",
      "scrapeOptions": {
      "formats": ["markdown"],
      "onlyMainContent": true
    }
  }
  ```

  **Returns:**
  - Array of search results (with optional scraped content)

  **Prompt Example:**

  > "Find the latest research papers on AI published in 2023."

  ### 6. Crawl Tool (`firecrawl_crawl`)

  Starts an asynchronous crawl job on a website and extract content from all pages.

  **Best for:**
  - Extracting content from multiple related pages, when you need comprehensive coverage.

  **Not recommended for:**
  - Extracting content from a single page (use scrape)
  - When token limits are a concern (use map + batch_scrape)
  - When you need fast results (crawling can be slow)

  **Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.

  **Common mistakes:**
  - Setting limit or maxDepth too high (causes token overflow)
  - Using crawl for a single page (use scrape instead)

  **Prompt Example:**
  > "Get all blog posts from the first two levels of example.com/blog."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_crawl",
    "arguments": {
      "url": "https://example.com/blog/*",
      "maxDepth": 2,
      "limit": 100,
      "allowExternalLinks": false,
      "deduplicateSimilarURLs": true
    }
  }
  ```

  **Returns:**
  - Response includes operation ID for status checking:

  ```json
  {
    "content": [
      {
        "type": "text",
        "text": "Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress."
      }
    ],
    "isError": false
  }
  ```

  ### 7. Check Crawl Status (`firecrawl_check_crawl_status`)

  Check the status of a crawl job.

  ```json
  {
    "name": "firecrawl_check_crawl_status",
    "arguments": {
      "id": "550e8400-e29b-41d4-a716-446655440000"
    }
  }
  ```

  **Returns:**
  - Response includes the status of the crawl job:

  ### 8. Extract Tool (`firecrawl_extract`)

  Extract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.

  **Best for:**
  - Extracting specific structured data like prices, names, details.

  **Not recommended for:**
  - When you need the full content of a page (use scrape)
  - When you're not looking for specific structured data

  **Arguments:**
  - `urls`: Array of URLs to extract information from
  - `prompt`: Custom prompt for the LLM extraction
  - `systemPrompt`: System prompt to guide the LLM
  - `schema`: JSON schema for structured data extraction
  - `allowExternalLinks`: Allow extraction from external links
  - `enableWebSearch`: Enable web search for additional context
  - `includeSubdomains`: Include subdomains in extraction

  For extraction, the MCP server will use it uses Firecrawl's managed LLM service.

  **Prompt Example:**
  > "Extract the product name, price, and description from these product pages."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_extract",
    "arguments": {
      "urls": ["https://example.com/page1", "https://example.com/page2"],
      "prompt": "Extract product information including name, price, and description",
      "systemPrompt": "You are a helpful assistant that extracts product information",
      "schema": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "price": { "type": "number" },
        "description": { "type": "string" }
      },
      "required": ["name", "price"]
    },
    "allowExternalLinks": false,
    "enableWebSearch": false,
    "includeSubdomains": false
  }
  ```

  **Returns:**
  - Extracted structured data as defined by your schema

  ```json
  {
    "content": [
      {
        "type": "text",
        "text": {
          "name": "Example Product",
          "price": 99.99,
          "description": "This is an example product description"
        }
      }
    ],
    "isError": false
  }
  ```

  ### 9. Deep Research Tool (`firecrawl_deep_research`)

  Conduct deep web research on a query using intelligent crawling, search, and LLM analysis.

  **Best for:**
  - Complex research questions requiring multiple sources, in-depth analysis.

  **Not recommended for:**
  - Simple questions that can be answered with a single search
  - When you need very specific information from a known page (use scrape)
  - When you need results quickly (deep research can take time)

  **Arguments:**
  - `query`: The research question or topic to explore.
  - `maxDepth`: Maximum recursive depth for crawling/search (default: 3).
  - `timeLimit`: Time limit in seconds for the research session (default: 120).
  - `maxUrls`: Maximum number of URLs to analyze (default: 50).

  **Prompt Example:**
  > "Research the environmental impact of electric vehicles versus gasoline vehicles."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_deep_research",
    "arguments": {
      "query": "What are the environmental impacts of electric vehicles compared to gasoline vehicles?",
      "maxDepth": 3,
      "timeLimit": 120,
      "maxUrls": 50
    }
  }
  ```

  **Returns:**
  - Final analysis generated by an LLM based on research. (data.finalAnalysis)
  - May also include structured activities and sources used in the research process.

  ### 10. Generate LLMs.txt Tool (`firecrawl_generate_llmstxt`)

  Generate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact with the site.

  **Best for:**
  - Creating machine-readable permission guidelines for AI models.

  **Not recommended for:**
  - General content extraction or research

  **Arguments:**
  - `url`: The base URL of the website to analyze.
  - `maxUrls`: Max number of URLs to include (default: 10).
  - `showFullText`: Whether to include llms-full.txt contents in the response.

  **Prompt Example:**
  > "Generate an LLMs.txt file for example.com."

  **Usage Example:**

  ```json
  {
    "name": "firecrawl_generate_llmstxt",
    "arguments": {
      "url": "https://example.com",
      "maxUrls": 20,
      "showFullText": true
    }
  }
  ```

  **Returns:**
  - LLMs.txt file contents (and optionally llms-full.txt)

  ## Logging System

  The server includes comprehensive logging:

  - Operation status and progress
  - Performance metrics
  - Credit usage monitoring
  - Rate limit tracking
  - Error conditions

  Example log messages:

  ```
  [INFO] Firecrawl MCP Server initialized successfully
  [INFO] Starting scrape for URL: https://example.com
  [INFO] Batch operation queued with ID: batch_1
  [WARNING] Credit usage has reached warning threshold
  [ERROR] Rate limit exceeded, retrying in 2s...
  ```

  ### 11. Error Handling

  The server provides robust error handling:

  - Automatic retries for transient errors
  - Rate limit handling with backoff
  - Detailed error messages
  - Credit usage warnings
  - Network resilience

  Example error response:

  ```json
  {
    "content": [
      {
        "type": "text",
        "text": "Error: Rate limit exceeded. Retrying in 2 seconds..."
      }
    ],
    "isError": true
  }
  ```

metadata:
  categories: Retrieval & Search,Automation & Browsers,Science & Research
  source: vendor
icon: https://avatars.githubusercontent.com/u/135057108?v=4
repoURL: https://github.com/mendableai/firecrawl-mcp-server
toolPreview:
  - name: firecrawl_scrape
    description: Scrape content from a single URL with advanced options.
    params:
      actions: List of actions to perform before scraping
      excludeTags: HTML tags to exclude from extraction
      extract: Configuration for structured data extraction
      formats: 'Content formats to extract (default: [''markdown''])'
      includeTags: HTML tags to specifically include in extraction
      location: Location settings for scraping
      maxAge: 'Maximum age in milliseconds for cached content. Use cached data if available and younger than maxAge, otherwise scrape fresh. Enables 500% faster scrapes for recently cached pages. Default: 0 (always scrape fresh)'
      mobile: Use mobile viewport
      onlyMainContent: Extract only the main content, filtering out navigation, footers, etc.
      removeBase64Images: Remove base64 encoded images from output
      skipTlsVerification: Skip TLS certificate verification
      timeout: Maximum time in milliseconds to wait for the page to load
      url: The URL to scrape
      waitFor: Time in milliseconds to wait for dynamic content to load
  - name: firecrawl_map
    description: Map a website to discover all indexed URLs on the site.
    params:
      ignoreSitemap: Skip sitemap.xml discovery and only use HTML links
      includeSubdomains: Include URLs from subdomains in results
      limit: Maximum number of URLs to return
      search: Optional search term to filter URLs
      sitemapOnly: Only use sitemap.xml for discovery, ignore HTML links
      url: Starting URL for URL discovery
  - name: firecrawl_crawl
    description: Starts an asynchronous crawl job on a website and extracts content from all pages.
    params:
      allowBackwardLinks: Allow crawling links that point to parent directories
      allowExternalLinks: Allow crawling links to external domains
      deduplicateSimilarURLs: Remove similar URLs during crawl
      excludePaths: URL paths to exclude from crawling
      ignoreQueryParameters: Ignore query parameters when comparing URLs
      ignoreSitemap: Skip sitemap.xml discovery
      includePaths: Only crawl these URL paths
      limit: Maximum number of pages to crawl
      maxDepth: Maximum link depth to crawl
      scrapeOptions: Options for scraping each page
      url: Starting URL for the crawl
      webhook: ""
  - name: firecrawl_check_crawl_status
    description: Check the status of a crawl job.
    params:
      id: Crawl job ID to check
  - name: firecrawl_search
    description: Search the web and optionally extract content from search results.
    params:
      country: 'Country code for search results (default: us)'
      filter: Search filter
      lang: 'Language code for search results (default: en)'
      limit: 'Maximum number of results to return (default: 5)'
      location: Location settings for search
      query: Search query string
      scrapeOptions: Options for scraping search results
      tbs: Time-based search filter
  - name: firecrawl_extract
    description: Extract structured information from web pages using LLM capabilities.
    params:
      allowExternalLinks: Allow extraction from external links
      enableWebSearch: Enable web search for additional context
      includeSubdomains: Include subdomains in extraction
      prompt: Prompt for the LLM extraction
      schema: JSON schema for structured data extraction
      systemPrompt: System prompt for LLM extraction
      urls: List of URLs to extract information from
  - name: firecrawl_generate_llmstxt
    description: Generate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact with the site
    params:
      maxUrls: 'Maximum number of URLs to process (1-100, default: 10)'
      showFullText: Whether to show the full LLMs-full.txt in the response
      url: The URL to generate LLMs.txt from
env:
  - key: FIRECRAWL_API_KEY
    name: Firecrawl API Key
    required: true
    sensitive: true
    description: Your Firecrawl API key.
runtime: npx
npxConfig:
  package: firecrawl-mcp
